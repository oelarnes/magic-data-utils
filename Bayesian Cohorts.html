
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Inferring Win Rates using Bayes Theorem &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Bayesian Cohorts';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to your Jupyter Book" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Inferring Win Rates using Bayes Theorem</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FBayesian Cohorts.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Bayesian Cohorts.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Inferring Win Rates using Bayes Theorem</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayes Theorem</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="inferring-win-rates-using-bayes-theorem">
<h1>Inferring Win Rates using Bayes Theorem<a class="headerlink" href="#inferring-win-rates-using-bayes-theorem" title="Link to this heading">#</a></h1>
<p>I want to do something a little different and write up some mathematical results related to limited analysis in complete detail. The target for this is any student of mathematics that could use their interest in Magic as a bit of extra motivation. This is university-level probability theory, but calculus students might be able to follow along with some imagination. There will be a lot of math, and a little bit of scientific programming.</p>
<p>First, let’s set out the nature of the problem. We want to assign skill estimates to players with observed win rates that look like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">polars</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="kn">from</span> <span class="nn">spells</span> <span class="kn">import</span> <span class="n">summon</span><span class="p">,</span> <span class="n">ColType</span><span class="p">,</span> <span class="n">ColName</span><span class="p">,</span> <span class="n">ColSpec</span>
<span class="kn">from</span> <span class="nn">spells</span> <span class="kn">import</span> <span class="n">extension</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uncomment to run. This cell will use several GB of bandwidth and about a GB of storage on your machine to enable the below analysis. </span>
<span class="c1"># Probably better to do it from a real terminal.</span>

<span class="c1"># !spells add DSK</span>
<span class="c1"># !spells add BLB</span>
<span class="c1"># !spells add OTJ</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sets</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;DSK&quot;</span><span class="p">,</span> <span class="s2">&quot;BLB&quot;</span><span class="p">,</span> <span class="s2">&quot;OTJ&quot;</span><span class="p">]</span>
<span class="n">UWR</span> <span class="o">=</span> <span class="n">ColName</span><span class="o">.</span><span class="n">USER_GAME_WIN_RATE_BUCKET</span>
<span class="n">UNG</span> <span class="o">=</span> <span class="n">ColName</span><span class="o">.</span><span class="n">USER_N_GAMES_BUCKET</span>
<span class="n">ND</span> <span class="o">=</span> <span class="n">ColName</span><span class="o">.</span><span class="n">NUM_DRAFTS</span>
<span class="n">df10</span> <span class="o">=</span> <span class="n">summon</span><span class="p">(</span><span class="n">sets</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">group_by</span><span class="o">=</span><span class="p">[</span><span class="n">UWR</span><span class="p">],</span> <span class="n">filter_spec</span><span class="o">=</span><span class="p">{</span><span class="n">UNG</span><span class="p">:</span> <span class="mi">10</span><span class="p">})</span>
<span class="n">df50</span> <span class="o">=</span> <span class="n">summon</span><span class="p">(</span><span class="n">sets</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">group_by</span><span class="o">=</span><span class="p">[</span><span class="n">UWR</span><span class="p">],</span> <span class="n">filter_spec</span><span class="o">=</span><span class="p">{</span><span class="n">UNG</span><span class="p">:</span> <span class="mi">50</span><span class="p">})</span>

<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">df10</span><span class="p">[</span><span class="n">UWR</span><span class="p">]</span><span class="o">-</span><span class="mf">0.010</span><span class="p">,</span> <span class="n">df10</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">height</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;10&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">df50</span><span class="p">[</span><span class="n">UWR</span><span class="p">]</span><span class="o">-</span><span class="mf">0.00</span><span class="p">,</span> <span class="n">df50</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">height</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;50&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of drafts by win rate bucket for different N games cohorts&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eb638ab6830bbb067c0d81e302b40751d0195fff6ef5d0b28efbb8832e47f1fe.png" src="_images/eb638ab6830bbb067c0d81e302b40751d0195fff6ef5d0b28efbb8832e47f1fe.png" />
</div>
</div>
<p>This might not look so bad, but we see some very extreme win rates, north of 75%. And even the not so extreme ones, do we want to lump someone that went 6-3 with someone that went 600-300? In general, we have the intuition that we need some more evidence before we assign someone a high win rate. Can we quantify that systematically? Certainly! Can we do it simply, or do we need a comprehensive theory? We can do it simply, but today we are going to look at a comprehensive theory, that happens to have a simple and intuitive result.</p>
<p>Let’s consider the problem of estimating win rates. A win rate is a statistic (meaning, a computation on a data set) that is presumed to be driven by some <em>skill parameter</em> <span class="math notranslate nohighlight">\(\lambda\)</span>. Then the outcome of each game is a Bernoulli random variable
$<span class="math notranslate nohighlight">\(
W_n = \begin{cases} 1 &amp; \text{with probability} &amp; \lambda \\ 0 &amp; \text{with probability} &amp; 1-\lambda. \\ \end{cases}
\)</span>$</p>
<p>Bernoulli, meaning a guy that got nearly the simplest thing imaginable named after him, and random variable, meaning, a variable (<span class="math notranslate nohighlight">\(W\)</span>) which takes on some value or another according to a random distribution.  Both random and a variable, contrary to what you might have heard. Suppose we observe a series of <span class="math notranslate nohighlight">\(w\)</span> wins (<span class="math notranslate nohighlight">\(1\)</span>s) and <span class="math notranslate nohighlight">\(l\)</span> losses (<span class="math notranslate nohighlight">\(0\)</span>s) by politely and repeatedly asking <span class="math notranslate nohighlight">\(W\)</span> for it’s value. What is our estimate of <span class="math notranslate nohighlight">\(\lambda\)</span>? A straightforward thing to do is to take the mean, which is also just the observed win rate, and let
$<span class="math notranslate nohighlight">\(
\hat\lambda = \frac{w}{w+l}.
\)</span>$</p>
<p>e.g., 40% for 4 wins and 6 losses. But what if we observe a small number of games, and the ratio between <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(l\)</span> is extreme? We don’t presume that after we watch a player win one game that they are likely to win the rest. Instead, we assume that their skill parameter is likely close to average, perhaps a bit higher based on the result we just witnessed. So we would like to formalize that idea. Bayes suggests that instead of having a fixed parameter representing our knowledge of their skills, we instead choose a <em>prior distribution</em>, that is, create a random variable that represents our model of their skill, and update the distribution of that variable when we make observations. But how do we choose our prior? Let’s not. Let’s just suppose that <span class="math notranslate nohighlight">\(\Lambda\)</span> (now capitalized because it’s random) has a continuous distribution described by a density function <span class="math notranslate nohighlight">\(f_\Lambda: (0,1) \rightarrow \mathbb{R}^+\)</span>, meaning our random variable <span class="math notranslate nohighlight">\(\Lambda\)</span> takes on positive values between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. This makes sure we can land at any spot in that interval for our value <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> once we see enough games.</p>
<section id="bayes-theorem">
<h2>Bayes Theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading">#</a></h2>
<p>Bayes theorem states that for events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> under a probability measure <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>,
$<span class="math notranslate nohighlight">\(
\mathbf{P}(A|B) = \frac{\mathbf{P}(B|A) \cdot \mathbf{P}(A)}{\mathbf{P}(B)}.
\)</span>$</p>
<p>The bar means “conditional on”, so the interpretation is that our updated estimate of the likelihood of <span class="math notranslate nohighlight">\(B\)</span>, once we know whether or not <span class="math notranslate nohighlight">\(A\)</span> happened, can be derived if we know how to calculate the likelihood of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span>, and the underlying rates. That is exactly our situation. We want a whole distribution (for <span class="math notranslate nohighlight">\(\Lambda\)</span>), not just an event, but we will be a bit cheeky and just say <span class="math notranslate nohighlight">\(\mathbf{P}(\Lambda \in dx)\)</span> where <span class="math notranslate nohighlight">\(dx\)</span> is shorthand for the interval <span class="math notranslate nohighlight">\([x, x+dx]\)</span>, and this <span class="math notranslate nohighlight">\(dx\)</span> is shorthand for some number small enough to get the accuracy we need. See a probability textbook for a proper definition, but professionals do this kind of thing all the time. So this expression with the “in” sign <span class="math notranslate nohighlight">\(\in\)</span> is an event, either the value for <span class="math notranslate nohighlight">\(\Lambda\)</span> lands in that interval or it doesn’t. For a smooth distribution like the one we are working with (thanks to our choice of prior), this will be a function of <span class="math notranslate nohighlight">\(x\)</span> with a <span class="math notranslate nohighlight">\(dx\)</span> factor, which we can divide out to obtain the density function. Our other event is <span class="math notranslate nohighlight">\(W\)</span> (or <span class="math notranslate nohighlight">\(W^c\)</span>), the event of a win (or loss). Let’s rewrite the thing we want with our new variables:
$<span class="math notranslate nohighlight">\(
\mathbf{P}(\Lambda \in dx|W) = \frac{\mathbf{P}(W|\Lambda \in dx) \cdot \mathbf{P}(\Lambda \in dx)}{\mathbf{P}(W)}.
\)</span>$</p>
<p>So the left-hand side is what we want, it’s the new distribution for <span class="math notranslate nohighlight">\(\Lambda\)</span>. Well, it’s one event, but it tells us the value of a new density function at <span class="math notranslate nohighlight">\(x\)</span> describing our updated guess at <span class="math notranslate nohighlight">\(\Lambda\)</span> given the observation <span class="math notranslate nohighlight">\(W\)</span> (a win, in this case. We will have to derive both formulas).</p>
<p><span class="math notranslate nohighlight">\(\mathbf{P}(W|\Lambda \in dx)\)</span> looks intimidating, but the condition is that <span class="math notranslate nohighlight">\(\Lambda\)</span> is constrained to be very close to a fixed value <span class="math notranslate nohighlight">\(x\)</span>, and we know what to do when <span class="math notranslate nohighlight">\(\Lambda\)</span> is fixed. We check our Bernoulli distribution above and see that the value must be <span class="math notranslate nohighlight">\(x\)</span>. <span class="math notranslate nohighlight">\(\mathbf{P}(\Lambda \in dx)\)</span> is just our density function, or more properly, our density function times <span class="math notranslate nohighlight">\(dx\)</span>, so <span class="math notranslate nohighlight">\(f_\Lambda(x)dx\)</span>. That leaves <span class="math notranslate nohighlight">\(\mathbf{P}(W)\)</span>, which looks inoccuous, but actually is the only tricky thing here. We forgot to mention how to estimate the likelihood of winning for a random skill parameter! We’ll have to resort to what we know, which is the value given <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>With conditional probabilities you can do a trick where you wrap a conditional probability inside of another probability, like this:
$<span class="math notranslate nohighlight">\(
\mathbf{P}(W) = \mathbf{P}(\mathbf{P}(W|\Lambda)).
\)</span>$</p>
<p>Don’t forget this one! Yes I cheated and we’re conditioning on a variable, not an event, but it’s essentially the same object as the <span class="math notranslate nohighlight">\(dx\)</span> version. There’s no great way to write it otherwise. But you can follow this: the interpretation is, “the chance Anson wins the game is the same as the chance that Anson wins the game after I whisper his skill parameter in your ear”. Not exactly mind-blowing, but we can calculate the second version, because once we know the parameter we know the chance he will win, and we also have a distribution of the parameter so we can guess how often each different whisper will happen.</p>
<p>Now if the parameter is <span class="math notranslate nohighlight">\(\Lambda\)</span> (which, remember is a random variable <span class="math notranslate nohighlight">\(\textemdash\)</span> a variable which represents a number underneath a distribution), we check Bernoulli’s distribution again and the probability of winning is… <span class="math notranslate nohighlight">\(\Lambda\)</span>, right. So our formula becomes:
$<span class="math notranslate nohighlight">\(
\mathbf{P}(W) = \mathbf{P}(\Lambda).
\)</span>$</p>
<p>You’re used to seeing an <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> here, for expectation, but I learned probability from an edgelord so I do it this way. It’s just an integral sign anyway:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{P}(\Lambda) = \int_0^1{xf_\Lambda(x)dx}.
\]</div>
<p>That’s the expectation of <span class="math notranslate nohighlight">\(\Lambda\)</span> under the probability measure given by the density function <span class="math notranslate nohighlight">\(f_\Lambda\)</span>. It’s notation, not a formula. Don’t forget the <span class="math notranslate nohighlight">\(x\)</span>! That’s the <span class="math notranslate nohighlight">\(\Lambda\)</span> in the <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> version. Now we can write down our whole formula:
$<span class="math notranslate nohighlight">\(
\mathbf{P}(\Lambda \in dx|W) = \frac{xf_\Lambda(x)dx}{\int_0^1{xf_\Lambda(x)dx}}.
\)</span>$</p>
<p>Uh, ok. Did we do anything here? Looks like <span class="math notranslate nohighlight">\(1\)</span>? Well it is, in an important way. Throw another integral sign there, and you do get <span class="math notranslate nohighlight">\(1\)</span>, which means we have ourselves a probability density function, and that means we have a new distribution! It’s also a good reminder that you often don’t need the denominator to work with Bayes’ rule, which is nice because it’s usually the hardest part to calculate.</p>
<p>Let’s start giving out distributions names, let’s identify them by the number of wins and losses observed, like this:
$<span class="math notranslate nohighlight">\(
f_{1,0}(x) = \frac{xf_{0,0}(x)}{\int_0^1{xf_{0,0}dx}}.
\)</span><span class="math notranslate nohighlight">\(
We divided out the \)</span>dx$, we didn’t forget it.</p>
<p>The denominator is going to start getting annoying, so I’m just going to leave it out and use a <span class="math notranslate nohighlight">\(\sim\)</span> symbol to denote proportionality. Go back and repeat the argument with a loss, and we get the formula
$<span class="math notranslate nohighlight">\(
f_{0,1}(x) \sim (1-x)f_{0,0}(x).
\)</span>$</p>
<p>Now, the whole idea is that our new distribution is our prior next time we make an observation. So if we see a win after that first loss, we can do this:
$<span class="math notranslate nohighlight">\(
f_{1,1}(x) \sim xf_{0,1}(x) \sim x(1-x)f_{0,0}(x).
\)</span>$</p>
<p>Rack up <span class="math notranslate nohighlight">\(w\)</span> wins and <span class="math notranslate nohighlight">\(l\)</span> losses, and we have
$<span class="math notranslate nohighlight">\(
f_{w,l}(x) \sim x^w(1-x)^lf_{0,0}(x).
\)</span>$</p>
<p>This is almost beautiful! It’s just that ugly <span class="math notranslate nohighlight">\(f_{0,0}\)</span> to be dispensed with. If we see enough data, can’t we forget about it anyway? Let’s just think about this as a family of functions. You get some <span class="math notranslate nohighlight">\(x\)</span>’s and some <span class="math notranslate nohighlight">\((1-x)\)</span>’s and maybe that’s all you need. Let <span class="math notranslate nohighlight">\(f_{0,0}(x) = 1\)</span>, which is a nice distribution called the <em>uniform distribution</em>, and we are left with this:
$<span class="math notranslate nohighlight">\(
f_{w,l}(x) = \frac{x^w(1-x)^l}{Z}.
\)</span><span class="math notranslate nohighlight">\(
\)</span>Z<span class="math notranslate nohighlight">\( just means whatever we need to divide by to get a total probability of \)</span>1$. It doesn’t matter that the uniform distribution is a lousy assumption about player skill, because we aren’t going to use it. We’re going to use a member of our new family, because once we can reason about the members of that family we can choose an appropriate one as our prior.</p>
<p>I guess we should learn how to reason about these distributions. Let’s graph a few of them. Let’s see what happens to the distribution over the sequence <span class="math notranslate nohighlight">\((W, W, L, L)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">f_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># our uniform prior</span>

<span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="s2">&quot;should sum to N&quot;</span>
    <span class="k">return</span> <span class="n">f</span> <span class="o">/</span> <span class="n">f</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">N</span>

<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">result</span><span class="p">):</span>
    <span class="s2">&quot;tell me this isn&#39;t a nice update function&quot;</span>
    <span class="k">return</span> <span class="n">normalize</span><span class="p">((</span><span class="n">x</span> <span class="k">if</span> <span class="n">result</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="n">prior</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

<span class="n">f</span><span class="o">=</span><span class="n">f_0</span>
<span class="n">wl_seq</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Uniform prior&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">wl_seq</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">res</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;After result </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Bayesian estimates of skill parameter&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1e435812ddcfe36a8b626fb24d749dbe5d5e35e9562c368c96037d864e63e803.png" src="_images/1e435812ddcfe36a8b626fb24d749dbe5d5e35e9562c368c96037d864e63e803.png" />
</div>
</div>
<p>After the second result, the green line, it looks like we’re leaning towards a skill parameter of <span class="math notranslate nohighlight">\(1.0\)</span>. While that’s not realistic in real life, remember that our prior likes <span class="math notranslate nohighlight">\(1.0\)</span> just as much as any other number, so it makes sense that we lean that way after a series of wins. A few wins and losses, and we are starting to get something with a centered shape. Let’s look at a few for larger random samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">lam</span> <span class="o">=</span> <span class="mf">0.54</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">lam</span>
    
    <span class="n">f</span><span class="o">=</span><span class="n">f_0</span>
    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Distribution after </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1"> random results&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Distributions for random results with a true skill parameter of </span><span class="si">{</span><span class="n">lam</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1a2a87e53dec613f371c9ec24df54268e16522e42f23bee88f5585d45de96545.png" src="_images/1a2a87e53dec613f371c9ec24df54268e16522e42f23bee88f5585d45de96545.png" />
</div>
</div>
<p>It takes time to build confidence! Even after 100 results, it looks like our estimate could range anywhere in a ~20% window based on the random results we observed. After 1000 we are starting to build some confidence, but it’s hard to quantify based on the graph alone. Remember, this is a theoretical distribution based on a single random sample, not a distribution of random results. This is the beauty of Bayesian statistics, that our posterior distribution can encode the uncertainty of our random process.</p>
<p>OK, but looking at the graph is one thing, we’d like to know what the distribution’s mean and variance is. Remember, the mean is also the win rate estimate, so that’s important. And we are going to use the variance to choose a prior. Let’s calculate. Recall formulas for mean and variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mu(\Lambda) &amp;= \mathbf{P}(\Lambda) = \int_0^1{xf_\Lambda(x)dx}\\
\text{Var}(\Lambda) &amp;= \mathbf{P}(\Lambda^2) - \mu(\Lambda)^2 = \int_0^1{x^2f_\Lambda(x)dx} - \mu(\Lambda)^2.
\end{align}
\end{split}\]</div>
<p>So to calculate <span class="math notranslate nohighlight">\(\mu\)</span> with our indexed probability distributions, we get</p>
<div class="math notranslate nohighlight">
\[
\mu_{w,l} = \frac{\int_0^1{x^{w+1}(1-x)^l dx}}{\int_0^1{x^{w}(1-x)^l dx}}.
\]</div>
<p>Looks like we will have to remember our integral calculus. What’s the one where you split the expression in half and take the derivative of one side? That’s right, integration by parts. Assume <span class="math notranslate nohighlight">\(l \geq 1, k \geq 0\)</span> and Let
$<span class="math notranslate nohighlight">\(
\begin{align}
u &amp;= (1-x)^l &amp; du &amp;= -l(1-x)^{l-1} \\
dv &amp;= x^k &amp; v &amp;= \frac{1}{k+1}x^{k+1}.\\
\end{align}
\)</span>$</p>
<p>Then
$<span class="math notranslate nohighlight">\(
\int_0^1{x^k(1-x)^l dx} = \frac{1}{k+1}x^{k+1}(1-x)^l \Bigr]_0^1 - \int_0^1{\frac{-l}{k+1}x^{k+1}(1-x)^{l-1}dx} = \frac{l}{k+1}\int_0^1{x^{k+1}(1-x)^{l-1}dx}.
\)</span>$</p>
<p>We recursively work our way down to <span class="math notranslate nohighlight">\(l=0\)</span> and get
$<span class="math notranslate nohighlight">\(
\frac{l!}{(k+l) \cdots (k+1)} \int_0^1 x^{k+l}dx = \frac{l!}{(k + l +1) \cdots (k+1)}.
\)</span>$</p>
<p>Substitute <span class="math notranslate nohighlight">\(\mu_{w,l}\)</span> and we get
$<span class="math notranslate nohighlight">\(
\mu_{w,l} = \frac{l!}{(w+l+2) \cdots (w+2)} \cdot \frac{(w+l+1) \cdots (w+1)}{l!} = \frac{w+1}{w+l+2}.
\)</span><span class="math notranslate nohighlight">\(
You can verify that this works for \)</span>l=0$ as well.</p>
<p>Well this is particularly nice. It looks just like our sample win rate calculation with a bit extra to keep from locking ourselves in at <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. In fact, we can think of the uniform distribution as supplying us with an extra win and a loss, so if we say <span class="math notranslate nohighlight">\(w_0 = 1\)</span> and <span class="math notranslate nohighlight">\(l_0 = 1\)</span>, then we get
$<span class="math notranslate nohighlight">\(
\hat\lambda = \mu_{w,l} = \frac{w+w_0}{w+w_0+l+l_0}.
\)</span>$</p>
<p>We’re almost ready to choose a sensible prior. It’s not hard to see that we can pick arbitrary values for <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(l_0\)</span> (even non-integers! They won’t bite. If you notice that the formula is well-behaved in <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(l\)</span> even though we used factorial calculations in our derivation, you are beginning to get the idea of the <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>), and there’s a member of our distribution family that corresponds that that choice when used as a prior.</p>
<p>So what we’ve found, after all this calculation, is that choosing a Bayesian prior for skill estimation is equivalent to preloading our sample with some wins and losses. I think that is a very cool result. But we’re not done. How do we choose <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(l_0\)</span>? Well, we should pick a sensible rate to start. For 17Lands data, users tend to have a winrate of about 54%, so we’ll let <span class="math notranslate nohighlight">\(w_0 = 0.54(w_0 + l_0)\)</span>. To choose the prior sample size (that is, <span class="math notranslate nohighlight">\(w_0 + l_0\)</span>), we should look at the expected variance of skill. We want our prior to represent the distribution of skill in the population, which we can do by matching variance.</p>
<p>Let’s look at one bucket of users to simplify things, those with 50-100 games. We’ll assume that they played 75 games each, so the variance in their observed win rate, assuming fixed skill, should be
$<span class="math notranslate nohighlight">\(
\text{Var} = 0.54 \cdot 0.46 / 75 \sim 3.3 * 10^{-3}.
\)</span>$</p>
<p>Let’s see what the sample gives us. I have a built-in function to get the pool-weighted variance of any metric, which will do fine for this. Strictly speaking, we should draft-weight, but we don’t need an exact answer here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wr_stats</span> <span class="o">=</span> <span class="n">extension</span><span class="o">.</span><span class="n">stat_cols</span><span class="p">(</span><span class="n">UWR</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">var_col</span> <span class="o">=</span> <span class="s2">&quot;user_game_win_rate_bucket_pw_var&quot;</span>
<span class="n">summon</span><span class="p">(</span><span class="n">sets</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">var_col</span><span class="p">],</span> <span class="n">group_by</span><span class="o">=</span><span class="p">[</span><span class="n">UWR</span><span class="p">],</span> <span class="n">filter_spec</span><span class="o">=</span><span class="p">{</span><span class="n">UNG</span><span class="p">:</span><span class="mi">50</span><span class="p">},</span> <span class="n">extensions</span><span class="o">=</span><span class="n">wr_stats</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="n">var_col</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (1,)</small><table border="1" class="dataframe"><thead><tr><th>user_game_win_rate_bucket_pw_var</th></tr><tr><td>f64</td></tr></thead><tbody><tr><td>0.00546</td></tr></tbody></table></div></div></div>
</div>
<p>The number of samples and the variance in skill are nearly independent sources of randomness, so it will be fine to subtract here. Our estimate of the variance of the distribution of player skill (for this bucket anyway) is then about <span class="math notranslate nohighlight">\(0.0022\)</span>. Oh, we forgot to calculate the variance of our distribution. Let’s do that now.</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(\Lambda) = \mathbf{P}(\Lambda^2) - \mu(\Lambda)^2,
\]</div>
<p>and
$<span class="math notranslate nohighlight">\(
\begin{align}
\mathbf{P}(\Lambda^2) &amp;= \frac{\int_0^1{x^{w+2}(1-x)^l dx}}{\int_0^1{x^{w}(1-x)^l dx}}
    &amp;= \frac{l!}{(w+l+3) \cdots (w+3)} \cdot \frac{(w+l+1) \cdots (w+1)}{l!}
    &amp;= \frac{(w+2)(w+1)}{(w+l+3)(w+l+2)},
\end{align}
\)</span><span class="math notranslate nohighlight">\(
so
\)</span><span class="math notranslate nohighlight">\(
\begin{align}
\text{Var}(\Lambda) &amp;= \frac{(w+2)(w+1)}{(w+l+3)(w+l+2)} - \frac{(w+1)^2}{(w+l+2)^2} = \frac{(w+1)(l+1)}{(w+l+2)(w+l+2)(w+l+3)},
\end{align}
\)</span>$
which you can expand out and verify.</p>
<p>It’s not pretty, but when you realize the heavy lifting the formula is doing, the elegance comes into view. Here are a few facts this formula should satisfy:</p>
<ul class="simple">
<li><p>For the uniform distribution (<span class="math notranslate nohighlight">\(w = l = 0\)</span>), we should get <span class="math notranslate nohighlight">\(\int_{-1/2}^{1/2}x^2dx = 1/12\)</span>. Check.</p></li>
<li><p>It should scale like <span class="math notranslate nohighlight">\(1/n\)</span>, since it represents our uncertainty after <span class="math notranslate nohighlight">\(n\)</span> observations. Check.</p></li>
<li><p><span class="math notranslate nohighlight">\(n * \text{Var}\)</span> should converge to <span class="math notranslate nohighlight">\(\lambda * (1-\lambda\)</span>) for large <span class="math notranslate nohighlight">\(n\)</span>. Also check.</p></li>
</ul>
<p>The reasons for this I will leave as an excercise! But we have a formula, so we can convert our variance to a sample size. Solve</p>
<div class="math notranslate nohighlight">
\[
0.0022 = \frac{0.54 \cdot 0.46} {n+1},
\]</div>
<p>(look familiar?), and we get… wait…</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="mf">0.54</span> <span class="o">*</span> <span class="mf">0.46</span> <span class="o">/</span> <span class="mf">0.0022</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>111.90909090909092
</pre></div>
</div>
</div>
</div>
<p>112 games!</p>
<p>So that’s it, our final estimator of win rate for a player with <span class="math notranslate nohighlight">\(w\)</span> wins and <span class="math notranslate nohighlight">\(l\)</span> losses, or a player with <span class="math notranslate nohighlight">\(\bar{\lambda}\)</span> win rate and <span class="math notranslate nohighlight">\(n\)</span> games played, is</p>
<div class="math notranslate nohighlight">
\[
\hat{\lambda} = \frac{w + 0.54 * 112}{w + l + 112} = \frac{\bar{\lambda} * n + 0.54 * 112}{n + 112} = 0.54 + (\bar\lambda - 0.54) * \frac{n}{n+112}.
\]</div>
<p>I use the last formula because it’s a bit more convenient as a one-liner. Hmm, shouldn’t we consider the effect of rank? Another exercise for the reader…</p>
<p>Let’s see what that looks like mapped to our cohorts. I’m going to pick some values of average games played by each n-games bucket. We could do this scientifically but I’ve had enough science for one day.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BAYES_MU</span> <span class="o">=</span> <span class="mf">0.54</span>
<span class="n">BAYES_N</span> <span class="o">=</span> <span class="mi">112</span>

<span class="n">ext</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;bayes_wr&#39;</span><span class="p">:</span> <span class="n">ColSpec</span><span class="p">(</span>
        <span class="n">col_type</span><span class="o">=</span><span class="n">ColType</span><span class="o">.</span><span class="n">GROUP_BY</span><span class="p">,</span>
        <span class="n">expr</span><span class="o">=</span><span class="p">((</span><span class="n">pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">UNG</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="mi">1200</span><span class="o">/</span><span class="p">(</span><span class="mi">1200</span> <span class="o">+</span> <span class="n">BAYES_N</span><span class="p">))</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">UNG</span><span class="p">)</span> <span class="o">==</span> <span class="mi">500</span><span class="p">)</span><span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="mi">750</span><span class="o">/</span><span class="p">(</span><span class="mi">750</span><span class="o">+</span><span class="n">BAYES_N</span><span class="p">))</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">UNG</span><span class="p">)</span> <span class="o">==</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="mi">300</span><span class="o">/</span><span class="p">(</span><span class="mi">300</span><span class="o">+</span><span class="n">BAYES_N</span><span class="p">))</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">UNG</span><span class="p">)</span> <span class="o">==</span> <span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="mi">75</span><span class="o">/</span><span class="p">(</span><span class="mi">75</span><span class="o">+</span><span class="n">BAYES_N</span><span class="p">))</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">UNG</span><span class="p">)</span> <span class="o">==</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">then</span><span class="p">(</span><span class="mi">30</span><span class="o">/</span><span class="p">(</span><span class="mi">30</span><span class="o">+</span><span class="n">BAYES_N</span><span class="p">))</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="mi">0</span><span class="p">)))))</span>
                <span class="o">*</span> <span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">UWR</span><span class="p">)</span> <span class="o">-</span> <span class="n">BAYES_MU</span><span class="p">)</span> <span class="o">+</span> <span class="n">BAYES_MU</span><span class="p">)</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">()</span> <span class="o">/</span> <span class="mi">50</span>
    <span class="p">),</span>
<span class="p">}</span>

<span class="n">df10</span> <span class="o">=</span> <span class="n">summon</span><span class="p">(</span><span class="n">sets</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">group_by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bayes_wr&#39;</span><span class="p">],</span> <span class="n">filter_spec</span><span class="o">=</span><span class="p">{</span><span class="n">UNG</span><span class="p">:</span> <span class="mi">10</span><span class="p">},</span> <span class="n">extensions</span><span class="o">=</span><span class="n">ext</span><span class="p">)</span>
<span class="n">df50</span> <span class="o">=</span> <span class="n">summon</span><span class="p">(</span><span class="n">sets</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">group_by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bayes_wr&#39;</span><span class="p">],</span> <span class="n">filter_spec</span><span class="o">=</span><span class="p">{</span><span class="n">UNG</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span> <span class="n">extensions</span><span class="o">=</span><span class="n">ext</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">df10</span><span class="p">[</span><span class="s1">&#39;bayes_wr&#39;</span><span class="p">]</span><span class="o">-</span><span class="mf">0.010</span><span class="p">,</span> <span class="n">df10</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">height</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;10&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">df50</span><span class="p">[</span><span class="s1">&#39;bayes_wr&#39;</span><span class="p">]</span><span class="o">-</span><span class="mf">0.00</span><span class="p">,</span> <span class="n">df50</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">height</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;50&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of drafts by Bayes win rate bucket for different N games cohorts&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e12ca257a24bd7e7a27c2c6b7baeb4ff5ddc650cf2dd3537f02f3257a13cabdc.png" src="_images/e12ca257a24bd7e7a27c2c6b7baeb4ff5ddc650cf2dd3537f02f3257a13cabdc.png" />
</div>
</div>
<p>This looks much nicer! We have fewer groups with a more reasonable distribution, and we can observe more confidence in higher skill for some of the players with more reps. Let’s add a few more buckets to see the big picture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prop</span> <span class="o">=</span> <span class="n">summon</span><span class="p">(</span><span class="n">sets</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">group_by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bayes_wr&#39;</span><span class="p">],</span> <span class="n">extensions</span><span class="o">=</span><span class="n">ext</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span><span class="n">ND</span><span class="p">:</span> <span class="s1">&#39;_&#39;</span><span class="p">})</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cohort</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">summon</span><span class="p">(</span><span class="n">sets</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">group_by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bayes_wr&#39;</span><span class="p">],</span> <span class="n">filter_spec</span><span class="o">=</span><span class="p">{</span><span class="n">UNG</span><span class="p">:</span> <span class="n">cohort</span><span class="p">},</span> <span class="n">extensions</span><span class="o">=</span><span class="n">ext</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">prop</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bayes_wr&#39;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;bayes_wr&#39;</span><span class="p">]</span><span class="o">-</span><span class="mf">0.02</span> <span class="o">*</span> <span class="n">i</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">ND</span><span class="p">],</span> <span class="n">height</span> <span class="o">=</span> <span class="mf">0.02</span><span class="o">/</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">cohort</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of drafts by Bayes win rate bucket for different N games cohorts&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/69cf55c997caf1009d9726558a0ac7c472eb24a0d91e30d1ac93ee7620977183.png" src="_images/69cf55c997caf1009d9726558a0ac7c472eb24a0d91e30d1ac93ee7620977183.png" />
</div>
</div>
<p>Looks pretty good! Although that bimodal bit is pretty weird. I wonder what’s going on there…</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Welcome to your Jupyter Book</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayes Theorem</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>